#LLM
![[CAG Systems 2025-03-21 19.54.29.excalidraw]]
## How It Works â€“ Step by Step

1. ### ğŸ§  User Query
    
    - User asks a question or issues a command.
    
2. ### ğŸ§­ Context Determination Engine
    
    - Not a retriever, but logic/rules/metadata selectors.
    - E.g., you ask about a specific product, so the system picks that productâ€™s data directly.
    
3. ### ğŸ“¦ Context Extraction
    
    - Pull the exact info from a structured source:
        - Database
        - GraphQL API
        - JSON
        - Tags, metadata
    - No similarity search, just context targeting.
    
4. ### ğŸ“ Prompt Building
    
    - Combine the structured context with the original query.
    - â€œHereâ€™s what we know + hereâ€™s what the user asked.â€
    
5. ### ğŸ’¬ LLM Generation
    
    - LLM uses the provided context (in prompt) to generate a response.
    
6. ### ğŸ“¤ Response
    
    - Send it back to the user, possibly with source or context attached.

## ğŸ§  Example Use Case (CAG)

> **â€œWhat is the current status of my order?â€**

- System identifies this as an **"order tracking"** intent.
- Looks up the **userâ€™s latest order** from the database.
- Adds order status + tracking info into the prompt.
- LLM formats a nice human-friendly response:
    
    > *â€œYour order #1234 is currently out for delivery and should arrive by tomorrow.â€*
    

No vector search. No guesswork. Clean context.
